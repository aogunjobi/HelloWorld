{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb04b13-35a8-4a93-b269-866daea8aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo pymatgen matminer scikit-learn smact pandas atomate fireworks\n",
    "!pip install mp-api\n",
    "!pip install --upgrade mp-api\n",
    "!apt-get update\n",
    "!apt-get install -y texlive texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
    "!pip install dask[dataframe]\n",
    "!pip install dask[complete] dask-ml matminer\n",
    "!pip install joblib\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71976c5c-276f-4d93-b42f-c7e4d9226c7a",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba7bd6-5de5-46da-a4cd-59edc3aeecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### General imports ###\n",
    "import warnings  # Import the warnings module to manage warning messages\n",
    "import numpy as np  # Import NumPy library for numerical computations\n",
    "import pandas as pd  # Import Pandas library for data manipulation and analysis\n",
    "import sqlite3  # Import SQLite3 library for working with SQLite databases\n",
    "import os  # Import os module for operating system related functions\n",
    "from tqdm import tqdm  # Import tqdm library for progress bars\n",
    "import csv  # Import csv module for reading and writing CSV files\n",
    "from pymongo import MongoClient  # Import MongoClient class from pymongo module for working with MongoDB\n",
    "import re  # Import re module for regular expressions\n",
    "import json  # Import json module for working with JSON data\n",
    "from itertools import zip_longest, combinations, product  # Import itertools functions for iterating and generating combinations\n",
    "import multiprocessing  # Import multiprocessing module for parallel computing\n",
    "import scipy.stats as stats  # Import stats submodule from scipy library for statistical functions\n",
    "from scipy.integrate import simps  # Import simps function from scipy.integrate module for numerical integration\n",
    "import pylab as pl  # Import pylab module for MATLAB-like plotting\n",
    "\n",
    "### Matplotlib ###\n",
    "import matplotlib.pyplot as plt  # Import pyplot module from Matplotlib library for plotting\n",
    "import matplotlib as mpl  # Import matplotlib module for configuring Matplotlib's settings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Matplotlib settings for LaTeX rendering\n",
    "mpl.style.use('ggplot')  # Set the style of Matplotlib plots to 'ggplot'\n",
    "mpl.rcParams['text.usetex'] = True  # Enable LaTeX rendering for text in plots\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\boldmath'  # Enable LaTeX rendering for bold math symbols\n",
    "mpl.rc('font', **{'family': 'serif', 'serif': 'Times New Roman', 'weight': 'bold'})  # Set font properties for Matplotlib plots\n",
    "mpl.rc('lines', **{'linewidth': 4.0})  # Set default line width for plots\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)  # Set default figure size for plots\n",
    "mpl.rcParams[\"font.size\"] = \"24\"  # Set default font size for text\n",
    "clrs = ['#254654', '#219e8f', '#eac563', '#f6a25a', '#e96f4c']  # Define a list of color codes for plots\n",
    "mpl.rcParams['text.color'] = '#565555'  # Set default text color\n",
    "mpl.rcParams['axes.labelcolor'] = '#565555'  # Set color for axis labels\n",
    "mpl.rcParams['axes.labelweight'] = 'bold'  # Set weight for axis labels\n",
    "mpl.rcParams['xtick.color'] = '#565555'  # Set color for x-axis ticks\n",
    "mpl.rcParams['ytick.color'] = '#565555'  # Set color for y-axis ticks\n",
    "mpl.rc('mathtext', **{'fontset': 'custom', 'rm': 'Times New Roman',\n",
    "                      'it': 'Times New Roman:italic', 'bf': 'Times New Roman:bold'})  # Set font properties for math text in plots\n",
    "\n",
    "### Pymatgen imports ###\n",
    "from pymatgen.core.composition import Composition  # Import Composition class from pymatgen library for representing compositions\n",
    "from pymatgen.core.structure import Structure  # Import Structure class from pymatgen library for representing crystal structures\n",
    "from pymatgen.core.periodic_table import Specie  # Import Specie class from pymatgen library for representing chemical species\n",
    "from pymatgen.ext.matproj import MPRester  # Import MPRester class from pymatgen library for accessing the Materials Project API\n",
    "\n",
    "### Matminer imports ###\n",
    "from matminer.data_retrieval.retrieve_MP import MPDataRetrieval  # Import MPDataRetrieval class from matminer library for retrieving data from the Materials Project\n",
    "from matminer.featurizers import composition as cf  # Import composition submodule from matminer library for composition featurization\n",
    "from matminer.featurizers.conversions import StrToComposition  # Import StrToComposition function from matminer library for converting string representations of compositions to Composition objects\n",
    "from matminer.featurizers.base import MultipleFeaturizer  # Import MultipleFeaturizer class from matminer library for applying multiple featurizers\n",
    "\n",
    "### Sklearn imports ###\n",
    "from sklearn import ensemble  # Import ensemble module from scikit-learn library for ensemble learning\n",
    "from sklearn.linear_model import LinearRegression, Ridge  # Import LinearRegression and Ridge classes from scikit-learn library for linear regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Import mean_squared_error and r2_score functions from scikit-learn library for evaluating model performance\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, train_test_split, GridSearchCV  # Import cross-validation and model selection functions from scikit-learn library\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler class from scikit-learn library for feature scaling\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline class from scikit-learn library for building pipelines\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor  # Import ensemble regressor classes from scikit-learn library\n",
    "from sklearn.svm import SVR  # Import SVR class from scikit-learn library for support vector regression\n",
    "from sklearn.compose import ColumnTransformer  # Import ColumnTransformer class from scikit-learn library for preprocessing data\n",
    "from sklearn.impute import SimpleImputer  # Import SimpleImputer class from scikit-learn library for handling missing values\n",
    "\n",
    "### SMACT imports ###\n",
    "import smact  # Import smact library for oxides\n",
    "\n",
    "### Filter warnings messages from the notebook ###\n",
    "warnings.filterwarnings('ignore')  # Ignore all warning messages\n",
    "\n",
    "### Set pandas view options ###\n",
    "pd.set_option('display.width', 1000)  # Set the maximum width for displaying dataframes to 1000\n",
    "pd.set_option('display.max_columns', None)  # Display all columns when displaying dataframes\n",
    "pd.set_option('display.max_rows', None)  # Display all rows when displaying dataframes\n",
    "\n",
    "### Set up MP API ###\n",
    "api_key = os.environ.get('XXXX')  # Get the Materials Project API key from the environment variables\n",
    "mpr = MPDataRetrieval(api_key='XXXX')  # Create an instance of MPDataRetrieval class with the provided API key\n",
    "m = MPRester(api_key='XXXX')  # Create an instance of MPRester class with the provided API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15055f69-18a6-4c23-ac7e-888fa12b9e8c",
   "metadata": {},
   "source": [
    "## Workflow for Extracting Data from SQLite and Fetching Materials Project Data\r\n",
    "\r\n",
    "This code provides a complete workflow for uploading a SQLite database, extracting data from it, and fetching additional materials data from the Materials Project APIfurther analysis.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fc5f7-9040-4794-ad78-714103d0d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from mp_api.client import MPRester\n",
    "\n",
    "# Create a file upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.db',  # Accept only .db files\n",
    "    multiple=False  # Do not allow multiple file uploads\n",
    ")\n",
    "\n",
    "def handle_upload(change):\n",
    "    # Get the uploaded file\n",
    "    uploaded_file = next(iter(upload_widget.value.values()))\n",
    "    content = uploaded_file['content']\n",
    "    db_path = '/mnt/data/mp_gllbsc.db'\n",
    "    \n",
    "    # Save the uploaded file content to a local file\n",
    "    with open(db_path, 'wb') as f:\n",
    "        f.write(content)\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a query to select all data from the 'number_key_values' table\n",
    "    cursor.execute(\"SELECT * FROM number_key_values\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Initialize lists to hold data\n",
    "    mpids, database_ids, ind_gaps = [], [], []\n",
    "\n",
    "    # Initialize a dictionary to temporarily hold the data\n",
    "    temp_data = {}\n",
    "\n",
    "    # Initialize the final list to hold combined dictionaries\n",
    "    cmr_data = []\n",
    "\n",
    "    # Populate temp_data with 'mpid' and 'database_id' first\n",
    "    for row in rows:\n",
    "        if row[0] == 'mpid' and len(row) >= 3:\n",
    "            mpid = 'mp-' + str(int(row[1]))\n",
    "            database_id = row[2]\n",
    "            temp_data[mpid] = {'mpid': mpid, 'database_id': database_id}\n",
    "\n",
    "    # Match 'ind_gap' with the corresponding 'mpid'\n",
    "    for row in rows:\n",
    "        if row[0] == 'gllbsc_ind_gap' and len(row) >= 2:\n",
    "            ind_gap = row[1]\n",
    "            for mpid, data in temp_data.items():\n",
    "                if 'ind_gap' not in data:\n",
    "                    data['ind_gap'] = ind_gap\n",
    "                    cmr_data.append(data)\n",
    "                    break\n",
    "\n",
    "    # Print the final combined data (first 5 entries)\n",
    "    print(\"Combined data:\")\n",
    "    for entry in cmr_data[:5]:\n",
    "        print(entry)\n",
    "\n",
    "    # Extract mpids for use with the Materials Project API\n",
    "    mpids = [entry['mpid'] for entry in cmr_data]\n",
    "\n",
    "    # Define the API key for the Materials Project API\n",
    "    API_KEY = \"XXXX\"\n",
    "\n",
    "    # Fetch materials data using the new MPRester syntax and store it in a DataFrame\n",
    "    with MPRester(API_KEY) as mpr:\n",
    "        try:\n",
    "            dataset = mpr.summary.search(\n",
    "                material_ids=mpids,\n",
    "                fields=['material_id', 'formula_pretty', 'elements', 'formula_anonymous', 'band_gap', 'energy_above_hull']\n",
    "            )\n",
    "            dataset = pd.DataFrame([\n",
    "                {field: getattr(doc, field) for field in ['material_id', 'formula_pretty', 'elements', 'formula_anonymous', 'band_gap', 'energy_above_hull']}\n",
    "                for doc in dataset\n",
    "            ])\n",
    "            # Print the formatted dataset (first 5 rows)\n",
    "            print(\"\\nFormatted dataset (first 5 rows):\")\n",
    "            print(dataset.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Observe the upload widget for changes\n",
    "upload_widget.observe(handle_upload, names='value')\n",
    "\n",
    "# Display the upload widget\n",
    "display(upload_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2594d1a-e9a7-414e-869f-1a8f5875d919",
   "metadata": {},
   "source": [
    "## Data Preparation and Featurization\n",
    "\n",
    "This code block primarily focuses on preparing and featurizing a dataset of oxides for further analysis. It includes data cleaning, merging, filtering, and feature extraction steps using `matminer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c0443-4ff0-4041-b8a4-ade9d2187b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.featurizers.conversions import StrToComposition\n",
    "from matminer.featurizers.composition import Stoichiometry, ElementProperty, ValenceOrbital, IonProperty, BandCenter, AtomicOrbitals\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "\n",
    "# Rename some columns\n",
    "dataset = dataset.rename(columns={'band_gap': 'PBE_gap'})\n",
    "\n",
    "# Ensure the 'elements' column exists before applying the tuple function\n",
    "if 'elements' in dataset.columns:\n",
    "    dataset['elements'] = dataset['elements'].apply(tuple)\n",
    "else:\n",
    "    print(\"'elements' column not found in dataset\")\n",
    "\n",
    "# Ensure cmr_data_list is correctly structured\n",
    "print(\"\\nFirst 5 entries in cmr_data_list after renaming columns:\")\n",
    "for entry in cmr_data[:5]:\n",
    "    print(entry)\n",
    "\n",
    "# Put the gllbsc band gaps into the dataframe\n",
    "for index, row in dataset.iterrows():\n",
    "    for entry in cmr_data:\n",
    "        if row['material_id'] == entry['mpid']:\n",
    "            dataset.loc[index, 'gllbsc_gap'] = entry['ind_gap']\n",
    "\n",
    "API_KEY = \"XXXX\"\n",
    "\n",
    "# Get data from Materials Project on oxides we have in the gllbsc database\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    try:\n",
    "        oxide_mpids = mpr.summary.search(elements=['O'], material_ids=mpids, fields=['material_id'])\n",
    "        oxide_mpids_df = pd.DataFrame([{field: getattr(doc, field) for field in ['material_id']} for doc in oxide_mpids])\n",
    "        print(\"First 5 entries in oxide_mpids_df:\")\n",
    "        print(oxide_mpids_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Add an indicator to show which entries in the dataset are oxides\n",
    "dataset = dataset.merge(oxide_mpids_df[['material_id']], on='material_id', how='left', indicator=True)\n",
    "dataset['oxide'] = dataset['_merge'] == 'both'\n",
    "dataset = dataset.drop('_merge', axis=1)\n",
    "\n",
    "# Filter out NaNs in band gaps columns\n",
    "dataset = dataset[np.isfinite(dataset['gllbsc_gap'])]\n",
    "dataset = dataset[np.isfinite(dataset['PBE_gap'])]\n",
    "\n",
    "# Pull out oxides to separate dataframe\n",
    "oxides = dataset.loc[dataset['oxide'] == True]\n",
    "\n",
    "# Ensure oxides DataFrame has the necessary columns\n",
    "necessary_columns = ['material_id', 'formula_pretty', 'elements', 'formula_anonymous', 'PBE_gap', 'gllbsc_gap', 'energy_above_hull']\n",
    "oxides = oxides[necessary_columns]\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(len(oxides))\n",
    "print(oxides.head())\n",
    "\n",
    "# Drop some gases and compounds in unusual oxidation states (inc peroxides)\n",
    "mask = oxides['formula_pretty'].isin([\n",
    "    'Mg(PO3)2', 'RbPO3', 'Na2Ca(PO3)4', 'Zn(PO3)2', 'Sr3BPO3', 'Ba3BPO3',\n",
    "    'AsPO5', 'SrBPO5', 'TiSO5',\n",
    "    'KAg(NO3)2', 'BrNO3', 'NaNO3', 'KNO3', 'AgNO3', 'TlNO3', 'LiNO3',\n",
    "    'B6O', 'Na2O2', 'Li2O2', 'CO2', 'SO2', 'BeO'\n",
    "])\n",
    "oxides = oxides[~mask]\n",
    "\n",
    "# Featurize the oxides DataFrame using matminer\n",
    "# Convert 'formula_pretty' to composition object\n",
    "oxides['composition_obj'] = StrToComposition().featurize_dataframe(oxides, 'formula_pretty')['composition']\n",
    "\n",
    "# Initialize the featurizer with the desired feature calculators\n",
    "feature_calculators = MultipleFeaturizer([\n",
    "    Stoichiometry(),\n",
    "    ElementProperty.from_preset(\"magpie\"),\n",
    "    ValenceOrbital(props=['avg']),\n",
    "    IonProperty(fast=True),\n",
    "    BandCenter(),\n",
    "    AtomicOrbitals()\n",
    "])\n",
    "\n",
    "# Featurize the DataFrame\n",
    "oxides = feature_calculators.featurize_dataframe(oxides, col_id='composition_obj')\n",
    "\n",
    "\n",
    "# Print the Featurized DataFrame\n",
    "print(len(oxides))\n",
    "print(oxides.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959cdc-4163-4661-8840-a6055db4b2c9",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor (GBR) Model for Predicting gllbsc_gap\n",
    "\n",
    "This code block sets up and trains a Gradient Boosting Regressor (GBR) model for predicting the gllbsc_gap of oxides. The RMSE score from 10-fold cross-validation is calculated. The preprocessing steps saved for consistent predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42d99e-1b67-4027-814a-5625b4f585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Alphabetize columns to make it easier later for use on new data\n",
    "oxides = oxides.reindex(sorted(oxides.columns), axis=1)\n",
    "\n",
    "# Choose columns to train on\n",
    "X_cols = [c for c in oxides.columns if c not in [\n",
    "    'material_id', 'formula_pretty', 'elements',\n",
    "    'formula_anonymous','PBE_gap', 'energy_above_hull',\n",
    "    'oxide', 'gllbsc_gap', 'composition_obj',\n",
    "    'HOMO_character', 'HOMO_element',\n",
    "    'LUMO_character', 'LUMO_element',\n",
    "    'avg s valence electrons'\n",
    "]]\n",
    "\n",
    "# Define the target and features\n",
    "y = oxides['gllbsc_gap'].values\n",
    "X = oxides[X_cols]\n",
    "\n",
    "# Ensure all features are numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Define cross-validation strategy\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Define the preprocessing and model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbr', GradientBoostingRegressor(\n",
    "        max_depth=20,\n",
    "        learning_rate=0.014485,\n",
    "        min_samples_split=65,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=86,\n",
    "        subsample=0.9,\n",
    "        n_estimators=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Save the scaler and imputer for use in the prediction phase\n",
    "joblib.dump(pipeline.named_steps['scaler'], 'scaler.pkl')\n",
    "joblib.dump(pipeline.named_steps['imputer'], 'imputer.pkl')\n",
    "\n",
    "# Get RMSE score from 10-fold CV\n",
    "if __name__ == '__main__':\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=5)\n",
    "    average_score = np.mean(scores)\n",
    "    print('GBR model RMSE: {}'.format(np.sqrt(abs(average_score))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bfb62-7a00-4cac-871f-41a057cd2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "\n",
    "# Extract the trained GBR model from the pipeline\n",
    "gbr = pipeline.named_steps['gbr']\n",
    "\n",
    "# Feature importances\n",
    "importances = gbr.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "x_imp = [X_cols[i] for i in indices]\n",
    "xtic = range(len(x_imp))\n",
    "y_imp = importances[indices]\n",
    "\n",
    "# Remove \"MagpieData \" prefix from labels if present\n",
    "cleaned_labels = [label.replace(\"MagpieData \", \"\") for label in x_imp]\n",
    "\n",
    "# Plot feature importances\n",
    "mpl.rcParams['figure.figsize'] = (12, 14)\n",
    "mpl.rcParams['text.usetex'] = True  # Enable LaTeX\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "mpl.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.bar(xtic[:20], y_imp[:20], color='royalblue', edgecolor='black')\n",
    "plt.xticks(xtic[:20], cleaned_labels[:20], rotation=90, fontsize=12)\n",
    "plt.yticks([])\n",
    "plt.ylabel('Relative importance', fontsize=16)\n",
    "plt.title('Top 20 Feature Importances', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('drive/MyDrive/Figures/Important_Features.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67070062-7951-48c5-a8a3-fb3cada802c4",
   "metadata": {},
   "source": [
    "## Training and Evaluating Gradient Boosting Regressor (GBR) Model with Varying Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d452b-4d10-46a3-a1b5-765f14919b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#from sklearn.model_selection import cross_val_score, KFold\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from tqdm import tqdm\n",
    "\n",
    "# Define cross-validation strategy\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize colors (clrs) if not defined\n",
    "clrs = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "# Generate models\n",
    "for n_est in tqdm(range(25, 2100, 25)):\n",
    "    gbr_model = GradientBoostingRegressor(\n",
    "        max_depth=20, learning_rate=0.01,\n",
    "        min_samples_split=65, min_samples_leaf=1,\n",
    "        max_features=86, subsample=0.9, n_estimators=n_est\n",
    "    )\n",
    "    gbr_model.fit(X, y)\n",
    "    y_pred = gbr_model.predict(X)\n",
    "    scores_train.append(np.sqrt(mean_squared_error(y_true=y, y_pred=y_pred)))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        scores = cross_val_score(gbr_model, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=5)\n",
    "        rmse_scores_gbr = [np.sqrt(abs(s)) for s in scores]\n",
    "        scores_test.append(np.mean(rmse_scores_gbr))\n",
    "\n",
    "# Plot\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "mpl.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(25, 2100, 25), scores_train, s=100, color=clrs[1], label='Training data')\n",
    "plt.scatter(range(25, 2100, 25), scores_test, s=100, color=clrs[3], label='Test data')\n",
    "plt.xlabel(r'\\textbf{Model complexity (number of decision trees)}', color='#565555', labelpad=20)\n",
    "plt.ylabel(r'\\textbf{RMSE (eV)}', color='#565555', labelpad=20)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('drive/MyDrive/Figures/test_vs_train.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a41095-99bd-4d73-a1d0-620d6f8d95e5",
   "metadata": {},
   "source": [
    "## Element Combinations and Compound Screening\r\n",
    "Combinations of elements are o generadte and screed s for potential compound formati kflow aims to identify viable compounds from combinations of selecte elements.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee23d41-5ddb-4fd9-aea1-bdcc9b3f637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import smact\n",
    "from smact import screening\n",
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b14d95-1db3-47fb-8c83-646d90b5cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smact import element_dictionary, neutral_ratios, screening\n",
    "from itertools import combinations, product\n",
    "\n",
    "# Define the elements we are interested in\n",
    "all_el = element_dictionary()  # Get a dictionary of all elements in SMACT\n",
    "symbol_list = [k for k, i in all_el.items()]  # Extract a list of element symbols from the dictionary\n",
    "\n",
    "# Define a list of elements we do not want to include\n",
    "do_not_want = ['H', 'He', 'B', 'C', 'O', 'Ne', 'Ar', 'Kr', 'Tc', 'Xe', 'Rn',\n",
    "               'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk',\n",
    "               'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr',\n",
    "               'Ra', 'Fr', 'At', 'Po', 'Pm', 'Eu', 'Tb', 'Yb']\n",
    "\n",
    "# Filter the list of elements to exclude the ones we do not want\n",
    "good_elements = [all_el[x] for x in symbol_list if x not in do_not_want]\n",
    "\n",
    "# Generate all combinations of three elements from the filtered list\n",
    "all_el_combos = list(combinations(good_elements, 3))  # Convert to list to get length for progress bar\n",
    "\n",
    "def smact_test(els):\n",
    "    all_compounds = []  # Initialize an empty list to store valid compounds\n",
    "    elements = [e.symbol for e in els] + ['O']  # Get the symbols of the elements in the combination and add 'O' for oxygen\n",
    "\n",
    "    # Get Pauling electronegativities of the three elements in the combination\n",
    "    paul_a, paul_b, paul_c = els[0].pauling_eneg, els[1].pauling_eneg, els[2].pauling_eneg\n",
    "    electronegativities = [paul_a, paul_b, paul_c, 3.44]  # Add the electronegativity of oxygen (3.44)\n",
    "\n",
    "    # For each set of species (in oxidation states) apply both SMACT tests\n",
    "    for ox_a, ox_b, ox_c in product(els[0].oxidation_states, els[1].oxidation_states, els[2].oxidation_states):\n",
    "        ox_states = [ox_a, ox_b, ox_c, -2]  # Create a list of oxidation states including -2 for oxygen\n",
    "\n",
    "        # Test for charge balance\n",
    "        cn_e, cn_r = neutral_ratios(ox_states, threshold=8)  # Check if the compound can be charge balanced\n",
    "\n",
    "        if cn_e:\n",
    "            # Electronegativity test\n",
    "            electroneg_OK = screening.pauling_test(ox_states, electronegativities)  # Check if the compound passes the Pauling electronegativity test\n",
    "            if electroneg_OK:\n",
    "                compound = tuple([elements, cn_r[0]])  # Create a tuple of the elements and their charge-neutral ratios\n",
    "                all_compounds.append(compound)  # Add the valid compound to the list\n",
    "    return all_compounds  # Return the list of valid compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea3406-070e-4258-8660-80f57af866ff",
   "metadata": {},
   "source": [
    "## Parallelized Screening of Element Combinations\n",
    "\n",
    "This parallelized approach significantly speeds up the screening process by utilizing multiple cores and provides real-time feedback on progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855733a-d3f7-4472-8376-5d6d4b538931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use a Pool for multiprocessing\n",
    "    with Pool() as p:\n",
    "        # Wrap the imap call with tqdm to add a progress bar\n",
    "        result = list(tqdm(p.imap(smact_test, all_el_combos, chunksize=10), total=len(all_el_combos)))\n",
    "\n",
    "    # Flatten the result list\n",
    "    flat_list = [item for sublist in result for item in sublist]\n",
    "\n",
    "    # Print the number of compositions\n",
    "    print(\"Number of compositions: {0}\".format(len(flat_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121413f-a3d3-435e-9c05-6b1cdca60e1e",
   "metadata": {},
   "source": [
    "## Featurization of Unique Compositions Using Matminer\n",
    "\n",
    "This code block uses multiprocessing and Dask to efficiently process and featurize a large dataset of unique compositions. The compositions are converted to features using Matminer’s `MultipleFeaturizer`, and the results are saved to a CSV file for further analysis.\n",
    "                                                                                                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5d5b6-368a-4e88-8366-c999218db07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core.composition import Composition\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db24aab-0c82-4ac0-bbb8-7baf96e87d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_maker(comp):\n",
    "    form = []\n",
    "    for el, ammt in zip(comp[0], comp[1]):\n",
    "        form.append(el)\n",
    "        form.append(ammt)\n",
    "    form = ''.join(str(e) for e in form)\n",
    "    pmg_form = Composition(form).reduced_formula\n",
    "    return pmg_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def9a3e-b645-4cd0-8208-2d6566fc7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pymatgen.core.composition import Composition\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    from matminer.featurizers.composition import MultipleFeaturizer, StrToComposition\n",
    "    import matminer.featurizers.composition as cf\n",
    "\n",
    "    # Use a Pool for multiprocessing\n",
    "    with Pool() as p:\n",
    "        pretty_formulas = p.map(comp_maker, flat_list)\n",
    "\n",
    "    # Convert to Dask DataFrame\n",
    "    unique_pretty_formulas = list(set(pretty_formulas))\n",
    "    print(\"Number of unique compositions formulas: {0}\".format(len(unique_pretty_formulas)))\n",
    "    new_data = pd.DataFrame(unique_pretty_formulas, columns=['pretty_formula'])\n",
    "    new_data_dd = dd.from_pandas(new_data, npartitions=10)\n",
    "\n",
    "    # Convert 'pretty_formula' to composition object using StrToComposition\n",
    "    str_to_comp = StrToComposition()\n",
    "    new_data_dd = str_to_comp.featurize_dataframe(new_data_dd, 'pretty_formula')\n",
    "\n",
    "    # Define feature calculators\n",
    "    feature_calculators = MultipleFeaturizer([cf.Stoichiometry(),\n",
    "                                              cf.ElementProperty.from_preset(\"magpie\"),\n",
    "                                              cf.ValenceOrbital(props=['avg']),\n",
    "                                              cf.IonProperty(fast=True),\n",
    "                                              cf.BandCenter(), cf.AtomicOrbitals()])\n",
    "\n",
    "    # Featurize dataframe\n",
    "    with ProgressBar():\n",
    "        new_data_dd = feature_calculators.featurize_dataframe(new_data_dd, col_id='composition', inplace=False)\n",
    "\n",
    "    # Compute and gather results\n",
    "    new_data = new_data_dd.compute()\n",
    "\n",
    "    # Describe the DataFrame\n",
    "    print(new_data.describe())\n",
    "\n",
    "    # Save as .csv file\n",
    "    new_data.to_csv('drive/MyDrive/data/All_oxide_comps_dataframe_featurized.csv', index=False, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c7d4b-bc1a-4c4b-841c-dc51fbafd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature names used during training\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    for item in X.columns:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88047e45-9908-4640-8a58-676975fa3bad",
   "metadata": {},
   "source": [
    "## Processing and Predicting Band Gaps for New Data\n",
    "\n",
    "This code block processes a large dataset in chunks, aligns the data with the trained feature set, imputes and scales the features, predicts band gaps using a pre-trained model, and saves the results to CSV files. It ensures efficient handling of large datasets by leveraging chunking, imputation, scaling, and model prediction while preserving original values where applicable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150c56e-4968-44ec-8bce-7acbae6f3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Load the trained scaler and imputer\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "imputer = joblib.load('imputer.pkl')\n",
    "\n",
    "# Load the feature names from the training phase\n",
    "with open('feature_names.txt', 'r') as f:\n",
    "    trained_feature_names = [line.strip() for line in f]\n",
    "\n",
    "# Define the path to the input and output files\n",
    "input_file = 'drive/MyDrive/data/All_oxide_comps_dataframe_featurized.csv'\n",
    "output_file_template = 'drive/MyDrive/data/results/new_data_with_predictions_part_{}.csv'\n",
    "\n",
    "# Load the original CSV to get the original values\n",
    "original_data = pd.read_csv(input_file)\n",
    "\n",
    "# Define a function to process a chunk of data\n",
    "def process_chunk(chunk, chunk_index):\n",
    "    # Drop the unnecessary column\n",
    "    chunk = chunk.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # Rename the columns\n",
    "    chunk = chunk.rename(columns={\n",
    "        'task_id': 'material_id',\n",
    "        'pretty_formula': 'formula_pretty',\n",
    "        'anonymous_formula': 'formula_anonymous',\n",
    "        'e_above_hull': 'energy_above_hull'\n",
    "    })\n",
    "\n",
    "    # Ensure columns match the trained features, preserving existing values\n",
    "    current_feature_names = chunk.columns.tolist()\n",
    "    missing_features = set(trained_feature_names) - set(current_feature_names)\n",
    "    additional_features = set(current_feature_names) - set(trained_feature_names)\n",
    "\n",
    "    # Print diagnostic information if there are mismatches\n",
    "    if missing_features or additional_features:\n",
    "        print(f\"Chunk {chunk_index} - Missing features: {missing_features}\")\n",
    "        print(f\"Chunk {chunk_index} - Additional features: {additional_features}\")\n",
    "\n",
    "    # Align the chunk with the trained features\n",
    "    for feature in missing_features:\n",
    "        chunk[feature] = 0\n",
    "\n",
    "    chunk = chunk[trained_feature_names]\n",
    "\n",
    "    # Define columns for prediction\n",
    "    X_cols_new_data = [c for c in trained_feature_names if c not in [\n",
    "        'material_id', 'formula_pretty', 'elements',\n",
    "        'formula_anonymous', 'PBE_gap', 'energy_above_hull',\n",
    "        'oxide', 'gllbsc_gap', 'composition_obj',\n",
    "        'HOMO_character', 'HOMO_element',\n",
    "        'LUMO_character', 'LUMO_element',\n",
    "        'avg s valence electrons'\n",
    "    ]]\n",
    "\n",
    "    # Impute missing values\n",
    "    X = imputer.transform(chunk[X_cols_new_data])\n",
    "\n",
    "    # Scale the features\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Use model to predict band gaps\n",
    "    chunk['gbr_gap'] = gbr.predict(X_scaled)\n",
    "\n",
    "    # Replace zero values with original values for columns with the MagpieData prefix\n",
    "    for col in chunk.columns:\n",
    "        original_col = col.replace(\"MagpieData \", \"\")\n",
    "        if original_col in original_data.columns and chunk[col].eq(0).any():\n",
    "            chunk[col] = chunk[col].mask(chunk[col].eq(0), original_data[original_col])\n",
    "\n",
    "    # Remove \"MagpieData \" prefix from column names after prediction\n",
    "    chunk.columns = [col.replace(\"MagpieData \", \"\") for col in chunk.columns]\n",
    "\n",
    "    # Save the chunk with predictions\n",
    "    chunk.to_csv(output_file_template.format(chunk_index), index=False)\n",
    "\n",
    "    # Explicitly clean up memory\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Read and process the data in chunks\n",
    "chunk_size = 10000  # Adjust the chunk size based on your memory constraints\n",
    "for chunk_index, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size)):\n",
    "    process_chunk(chunk, chunk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bee2a-eb74-4c4b-8474-dca7a64f72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "# Define the path to the output files\n",
    "output_file_pattern = 'drive/MyDrive/data/results/new_data_with_predictions_part_*.csv'\n",
    "\n",
    "# Verify that the files exist\n",
    "file_list = glob.glob(output_file_pattern)\n",
    "if not file_list:\n",
    "    raise FileNotFoundError(\"No prediction files found. Ensure the preprocessing step was successful.\")\n",
    "\n",
    "# Load the combined DataFrame with predictions\n",
    "new_data = dd.read_csv(output_file_pattern)\n",
    "\n",
    "# Filter based on band gap\n",
    "# 1.75 eV lies in the center of 1.0 - 2.5 eV (useful BG window)\n",
    "useful_BGs = new_data[(new_data['gbr_gap'] >= 1.73) & (new_data['gbr_gap'] <= 1.77)]\n",
    "\n",
    "# Compute the filtered DataFrame\n",
    "result = useful_BGs.compute()\n",
    "\n",
    "# Check if the result is empty\n",
    "if result.empty:\n",
    "    print(\"No useful band gaps found in the specified range.\")\n",
    "else:\n",
    "    print(result.head())\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    result.to_csv('drive/MyDrive/data/Bandgaps/filtered_useful_BGs.csv', index=False)\n",
    "    print(\"Filtered DataFrame saved to 'drive/MyDrive/data/Bandgaps/filtered_useful_BGs.csv'\")\n",
    "\n",
    "# Explicitly clean up memory\n",
    "del new_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb4d0b-cce5-4189-ae82-7ffa66399024",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
